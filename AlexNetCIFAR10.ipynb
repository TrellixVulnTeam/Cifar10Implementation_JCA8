{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DownloadProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\"\"\" \n",
    "    check if the data (zip) file is already downloaded\n",
    "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
    "\"\"\"\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    \n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "    \n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    \n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #3:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 994\n",
      "Label Counts of [1](AUTOMOBILE) : 1042\n",
      "Label Counts of [2](BIRD) : 965\n",
      "Label Counts of [3](CAT) : 997\n",
      "Label Counts of [4](DEER) : 990\n",
      "Label Counts of [5](DOG) : 1029\n",
      "Label Counts of [6](FROG) : 978\n",
      "Label Counts of [7](HORSE) : 1015\n",
      "Label Counts of [8](SHIP) : 961\n",
      "Label Counts of [9](TRUCK) : 1029\n",
      "\n",
      "Example of Image 9999:\n",
      "Image - Min Value: 3 Max Value: 242\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3WmQZGd15vHnZFbW2l29qdWNWmtrxQIEkkCbrY2xLMywSxP6AGgchrExMVgYTdhjwBY2REB4Ylht8JhFAThGECKQhzEGPEhCQpJtEAghCyEJdWvtfe/aM/PMh3sLqktZ3f2eys6sfuv/i+jIrsw8+b558+Z98uZyj7m7AABAnirdngAAADhyCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMhYT7cncCSY2QZJw5I2dnkqAABEnSxpr7ufMp8b6WrQm9nxkv5C0tWSVknaJOk2SR9w913zuOnhilVWDg4MrEwtnJqqJw9Wq1aTa6TY2ymVSuxNmIGhoVDdvpGR5JqJycnQWJWKBaoiNZKaHirr6+tLrumpxh4zrzeSa6rB9aPp6cvDLDZWdB2u9KZvrsbGJ0JjSenLo1arhUaKLo9ms5lcMzERe24ODQ4m1zQb6dtSSZoYGwvVRZ7RDU9fhlLs+eKBmsmpKXnonh2oa0FvZqdKulfSsZL+QdIjkl4h6Q8lXW1ml7j7juDNbxwcGFh57gtfmly4ddu25JrVQ8PJNZI0HHiBMDg4EBrr7Fe8PFR35w9+kFzz+FNPhsYa6E8PUWvGgr45GXuCn3rS+uSaY5fFXmRN7dyTXLOsP32DLEnjgY1yT09vaKy+pUtDdUPrVifX/PtjPw+N1Wikv8hat+640FgD/bHn9Pj4eHLNE088ERrr5eeel1wztmNnaKzHHvppqK5RSX9O75xI35GRpLH6VHLN1FR6zVNbNmlianJjcuEs3fyM/m9UhPy73P317v4n7n6lpI9KOlPSh7o4NwAAstCVoDez9ZKuUvEZ+l/PuvjPJY1IeouZxXaFAACApO7t0V9Znn7H/cAPSdx9n6R7JA1KurDTEwMAICfdCvozy9NH57j8sfL0jA7MBQCAbHXry3jLytO5vm00ff7yg92Imd0/x0VnRSYFAEBuFuoBc6a/Sj3/3xUAALCIdWuPfnqPfdkclw/Pul5L7t7yNx/lnv65sakBAJCPbu3RT/+4da7P4E8vT+f6DB8AAByGbgX9HeXpVTbr8FpmtlTSJZLGJP1LpycGAEBOuhL07v4LSd9RcRzfd866+AOShiR90d1jhy0CAACSunus+z9QcQjcT5jZKyX9TNIFkq5Q8Zb9e7s4NwAAstC1b92Xe/XnS7pZRcC/R9Kpkj4h6aJ5HOceAACUutq9zt2flvQ7R+K2zUw9gW5SPT3piyRSI0kW6Fo12UxvtiFJ9UCTDklSoONSb0+sScfSJWuTa1YtizVI2bb5mVBdrTf9Mav2xLob7hgfDYwVXRfTmwNF9xImAg1BJKka7IoYUQ80IOmtxZr8DAzEni8jgc6SY8HOcJFOeZEmLpIU7FMlD6zDHhwrsu2Odilsh4X6O3oAANAGBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMa62tTmyLJQs5lI4wGz2OulhtIbxkw1Yo0i6oGxJKleryfX9NViTTp6KkPJNWuOXRcay+vjobpGYyK5pq7Y8qjX0pvhjFVij3NPoLlHs5m+bkhSX6zHj3oH+pJrNmzcGBrrBccem1zT2xtrauOBxlFSrNHM9u3bQ2Nt3rQpuaZfsY4x0W3VZGB9HB2PbQc8cN8i21IPLovZ2KMHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMpZt9zqzWCe6CPf0LlKSNNFsJNfUg2NNNdLHkqTJycnkGgt2XKpU0+uGlsQ6wy0dTu+UJ0m7d25Jrmna8tBYYwp049q/OzSWTaaPtWJJ7H55M/aYWeD5vHfP3tBYLzz9jOSa4447LjRWdDs1ODiYXHPfvfeFxpoYT+/aGO3mN1GPdejcOzWSXLNvZH9orEqgY2mkm2qbmtexRw8AQM4IegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjGXb1KZa7dHKFSuS62q1WnJNf9OSayTJA01L1Bd7yKwSm+OSoSXJNVON9OYSkjRVT29Asm37M7GxpsZDdUuWpi+Pam/6OiVJ+ybGkmv27N0TGmtZtS+55tjVa0JjeTW2Lo5PpTdYWrIk1rxofCx92T/91NOhsao91VBds5ne4CrUWEVSf1/6+mEWe5yrtdgc9+9Pf05PBRqLSVI10Fwski3twh49AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADJG0AMAkLGuda8zs42STprj4i3uvnY+t9/b26sTTpjr5g+ikt7N6LF//2n6OIp1NTtx/WmhsZqN2Gu6448/Ibmmf9f20Fj7JybSa0Zj3dpWDi8L1a1ZdUxyTbOa3ulKkuo+lVwzMr4/NNaS5endyU487eTQWINLhkN1+8fT14+zzjo7NNb4aHr3uh8/8EBorB7zUN2atembyJNPPiU0Vi3QvW7ZsvROj5J0nAe225I2bEvvZGnB7nWNZvpjVutbnlwT7To6W7fb1O6R9LEW58e2VgAA4ADdDvrd7n5Tl+cAAEC2+IweAICMdXuPvs/M3izpREkjkh6UdJe7xz44AQAAB+h20K+V9KVZ520ws99x9+8dqtjM7p/jorPmPTMAADLQzbfuvyDplSrCfkjSiyX9raSTJf2TmZ3TvakBAJCHru3Ru/sHZp31kKTfN7P9kt4j6SZJbzjEbZzX6vxyT//cNkwTAICj2kL8Mt5nytNLuzoLAAAysBCDfmt5OtTVWQAAkIGFGPQXladPdHUWAABkoCtBb2Znm9nKFuefJOlT5Z9f7uysAADIT7e+jHetpD8xszskbZC0T9Kpkl4tqV/SNyX9jy7NDQCAbHQr6O+QdKakl6l4q35I0m5J31fxu/ovuXus0wMAAPilrgR9eTCcQx4QZz6q1aqWLF2aXLd7z5bkmkc2PJxcI0l1T//kpG/Z6tBYLzhmXahuONBpbOnwQGis/uXpj5fVY92d6qP1UN0xK1cl1zR60ruuSVKtmn7fmlOTobFGJkbSi/qqobGWrUjv4iVJzd3pva6OX3diaKyRkfTudWub46Gxqo30sSSpty/9ebZ0+HmfmB6WocH0sQZXxR7n6sjOUF1jMr3bY7UROwhrPbAfOjKWvn40A13yWlmIX8YDAABtQtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSsW93rOqJaTX8d8+yzzybX7N69J7lGkhqW3hRk165dobHWH39aqK4+kd5wo6+/LzTW8mUrkmt6VAuNtdtjj1lfLf2+jQZfTk8G+tPUR5uhsaY8fbANG9KfK5KknvTmRZI00JO+uRqoxRrv1JamN3Gp1paExqrUR0N1Xkm/byMTsSYpSwMNwurBhjHbdmwP1dXr6Y2qeizWFCuyFCen0ufXriau7NEDAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGcu2e93U1JQ2bX4uuW7jxo3pY03GujRV+tI7r01MTITGaga7IPX396fXxJrXqRbo5lfxWPepSrBrVSXQQW1iYjw01kD/cHLNC445PjRWXzN9eezfPxUa66ktW0N1a5eld1Dra6Z3DJOk8bH0ro27d6TXSNLSvti6OLw8vdtjdDsQsXPnjlBdZBssSRZ4TjdjzR7llr6P7IH96nY9WuzRAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBj2Ta1GRnZr3vvvTu5bsfO7ck1lWpvco0kNRrpLQtGR0ZDY42Nxur6aumvBa0Sa9IxGWrYk94IR5LGGrGGLP2e3sDo6ec2hcZqBBr2HH/SyaGxJvbuT67p6Yut97v27A7VLamlP9aNSqwtyHg9ff3YtWdPaKzKcKwLVM/4QHLNvrHYej8QeKzHx2LNnCYnJkN1HmjY02zG1g+vpkdntSe9iVmkUU8r7NEDAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGWtL9zozu0bSZZJeKukcSUsl/b27v/kgNRdLep+kCyX1S3pc0uclfdI90CJslsnJCT391JPJdZVK5LVP7PVSpDHR5FSss9OOHTtDdb3V9O5OAwOxjnK9A/3JNV6JrcJ9w0tDdctfcGxyzdrJkdBYe/aekFxz4nHrQmMdu+qY5Jqh5ctCYz397LOhuq2bNifXTIzHurVFmprVg5ut0bGxUN3W7Y8l1+zcH9t+bB4YTK7pX5r+fJak3mBXxP2BB80stu1uKH3j3VdLXx7R+c3Wrja171MR8PslPSPprINd2cxeJ+lrksYlfUXSTkmvkfRRSZdIurZN8wIAYFFr11v375Z0hqRhSe842BXNbFjS30lqSLrc3X/X3f+bincD7pN0jZld16Z5AQCwqLUl6N39Dnd/zN0P572TayStlnSLu/9wxm2Mq3hnQDrEiwUAAHB4uvFlvCvL02+1uOwuSaOSLjazvs5NCQCAPLXrM/oUZ5anj86+wN3rZrZB0tmS1kv62cFuyMzun+Oig35HAACAxaIbe/TTX9XdM8fl0+cv78BcAADIWjf26A9l+ncLh/y8393Pa3kDxZ7+ue2cFAAAR6Nu7NFP77HP9SPc4VnXAwAAQd0I+p+Xp2fMvsDMeiSdIqku6YlOTgoAgBx1I+hvL0+vbnHZpZIGJd3r7hOdmxIAAHnqRtDfKmm7pOvM7PzpM82sX9IHyz8/3YV5AQCQnXYd6/71kl5f/rm2PL3IzG4u/7/d3W+UJHffa2ZvVxH4d5rZLSoOgftaFT+9u1XFYXEBAMA8tetb9y+VdP2s89aX/yTpSUk3Tl/g7reZ2WWS3ivpTfpVU5s/kvSJwzzCHgAAOIS2BL273yTppsSaeyT9djvGb8lMVkvvomYWqIl+AhLoXlevxzpk7d4b+xFDxevpNdX0Gkladdya5Jor/sNvhcaa8sDClzQylv7VkdPPOC001svOOye5phl8jdzwZnLNmpUrQ2Nt3vJMqO6hhx9KrukfiHUp7Kmld1Crj8e6FI41x0N1zzy7Kbkm2MxP2wKrVf+ygdBYdY9NshroPBrpUhjlgedYu6ZHP3oAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZKxd3esWnJ6+fq0+9azkOkvvOxDuPGCBxiq13v7QWPum0puxSJI3J5NrevpiDWOuvuyi5JrTXnhGaKzv/vP3Q3W7d+1LrqlUYyvI2S96YXLNyP79obGefja90cxT/U+HxhpcsSxUt/IFq5Nrtm3fGRqrr5LePGqqnv5ckaSxvenrlCSNTwQaXEW2b5KagQ3jyN5Yc6tKT2z/s2K15JpmsIFOTyV9eTQmdqQP1Iwtw9nYowcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAylm33ukqlpqGlxyXXWSPQ3inYEWrpYHq3pRUrloTGWrZ8aaiuVkt/LXjS+hNCY73s3Jcm13ztq18LjXXf3T8M1Y3vT+92NdkcC43V15e+7J95Or0LnST94Ac/SK4ZGBoMjXXs2mNDdaeduj65ZmI8tuz37tubXLNkeCg0Vn9/NVRXraZ3iZwcGw+N1Wikd1FrRtt6BnlvIM4asWXf05Ne5wp0yrNYJ9DZ2KMHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMaybWpT66lpzar05hn7du9Orpkam0yukaSXnfPi5JrLLnt5aKwlS2INSHr70ps3RJt7qJI+1tKB2P069yXpy16SGlPpHYwmGiOhsdYcM5w+1misedFJJ6xJrmk0Y01LxvbtC9X1HZv+fH7tq14VGmvPvvTtwLpT0ptoSdKqFStDdaP79yfX7N6Vfr8kad9oenOgsZFYQ6Ht27aF6n6x4Ynkmj27Y9vuRqBsqp7eUGj3vv2q19MbCs3GHj0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQsbZ0rzOzayRdJumlks6RtFTS37v7m1tc92RJGw5yc19x9+vmP6em+nrTuyctOXYguea5p3ck10hSzSaSa3qDL81G9uwM1e2aSl+Gu3b0h8ZqePqd6+tJ73gnSVf8xsWhupXL0juNeTXWIWv/aKDLWz19nZKkl78svZvf5k2xLmO13tj6sfqYY5Jr1q8/NTRWrT99vepb1hsaq38gfZsjSZVK+vPFzUJjTUyldypsjse6ro0Euxs+ueW55Jpv/J/bQ2Pdfee/JtdUK+lxaxbbvs3Wrja171MR8PslPSPprMOo+Ymk21qc/1Cb5gQAwKLXrqB/t4qAf1zFnv0dh1HzgLvf1KbxAQBAC20Jenf/ZbBb8K0hAADQfu3ao484zsx+T9IqSTsk3efuD3ZxPgAAZKebQf+b5b9fMrM7JV3v7k8dzg2Y2f1zXHQ43xEAACB73fh53aikv5R0nqQV5b/pz/Uvl/RdMxvqwrwAAMhOx/fo3X2rpD+bdfZdZnaVpO9LukDS2yR9/DBu67xW55d7+ufOc6oAABz1FswBc9y9Lumz5Z+XdnMuAADkYsEEfWn6CBy8dQ8AQBsstKC/sDx9oquzAAAgEx0PejO7wMyed6xIM7tSxYF3JOnLnZ0VAAB5atex7l8v6fXln2vL04vM7Oby/9vd/cby/x+RdHb5U7pnyvNeIunK8v/vd/d72zEvAAAWu3Z96/6lkq6fdd768p8kPSlpOui/JOkNkl4u6VWSapK2SPqqpE+5+93tmNDU1Lg2PfNIct2Vl12eXDMxsiW5RpI2P7sxueZH/9oXGmvv3l2huo1Ppn+KYtXYG0WVWnqzk9F9I6GxHvrBj0J1g/3pc6z0xI4WWW+mNwXZtzfWEGT16tXJNT3VWmisWi3W/GXX8hXJNY//+8OhsayW/pjVlsSWR6Uaa1wyNLy0IzWSNBxo5rREwfXDYtuPY5ak37ddO2Lb7tHR9O3p4EBk2ac3E2qlXYfAvUnSTYd53c9J+lw7xgUAAAe30L6MBwAA2oigBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIWLu61y04A30DeuHpL06uG6yldxg6+4z0cSSpt5reia45FetmVAuMJUk7tu5Mrvn5Y4+Gxqr0pHc1+7WzzgyNteq4F4TqmvVmek2wG5dZelez5UtjneH27U7vAtiYmgqN5Y30rnyStL13U3JNvR4bq95oJNfs2rc7NNYjjz8WqrNa+ua7f+lQaKxfO+P05JqXnHhqaKzVy9M75UnSyvUnJNeccuKa0FjDgxck1zz55HPJNU89XtX4WHLZ87BHDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGQs2+514xMTevzRJ5LrmhPp3clOWx/r0rRlc3o3rmo19tqs2Yh1vdu8ZVdyTaUn1iHLK5Zc88yzm0NjXXrRhaG6wf707nCTsSZvagQeM/fY4zwyOppcs/W5Z0NjLV+5IlS3ZGl6Z8lmoAudJNWb6cvxF997MjTW1k3bQ3V9AwPJNRPBsZb1pI/16792fmiswcH0x1mSap4eZ5e94qLQWD996MHkGhsfT665P9ChsBX26AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQsWyb2nizqcmJseS6Sk96Y5VnN8eae2x8cmNyTV9fX2isqclYc4/9E/XkmjXrTgqN1Tc0mFzz2MPpzSUk6ZHHHg3VveCYlck19Ub6OiVJ7ul1FmgMJEm1nlpyzepjV8fGqsY2O2bp+yWV4K5Mo5HeiWj3/pHQWP0DsSZQxx9/fHLN5s2xJlB7du1Prtk7Eevm1BhNb/4iSfue25Zc07t7X2ispx97JrlmzZJlyTW1SjW5phX26AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMzbt7nZmtkvQGSa+W9GJJ6yRNSvqppC9I+oK7N1vUXSzpfZIulNQv6XFJn5f0SXePtVqbYXJyShs3pHeV270rvQPVypXpHc2kWCe6sdH0LlKSNDEZ6wjV25f+WtBs3g/fYXOP1X3/nvtCdUN96U8Zs/TOcJJUDXR5q9V6Q2NJ6Quypxa7X7JYh71qNb2TV6MZWxcnJ9O7Nu7YsSM01tBQrHvdunXrkmv27Yt1a9uxc2dyzUM/ezg01rrj0u+XJPX0pK8fe/aOhsYaG51IrlmxdEX6QIGOja20o03ttZI+LWmTpDskPSVpjaQ3SvqspFeZ2bXuv9okm9nrJH1N0rikr0jaKek1kj4q6ZLyNgEAwDy1I+gflfRaSf84c8/dzP5U0r9JepOK0P9aef6wpL+T1JB0ubv/sDz//ZJul3SNmV3n7re0YW4AACxq835fwN1vd/dvzH573t03S/pM+eflMy66RtJqSbdMh3x5/XEVb+VL0jvmOy8AAHDkv4w3VZ7O/MDryvL0Wy2uf5ekUUkXm1n6B9gAAOAA7XjrviUz65H01vLPmaF+Znn66Owad6+b2QZJZ0taL+lnhxjj/jkuOitttgAA5OlI7tF/WNKLJH3T3b894/xl5emeOeqmz19+pCYGAMBicUT26M3sXZLeI+kRSW9JLS9PD/l7H3c/b47x75d0buK4AABkp+179Gb2Tkkfl/SwpCvcffYPMKf32JepteFZ1wMAAEFtDXozu0HSpyQ9pCLkN7e42s/L0zNa1PdIOkXFl/eeaOfcAABYjNoW9Gb2xyoOePOAipDfOsdVby9Pr25x2aWSBiXd6+7phx4CAAAHaEvQlwe7+bCk+yW90t23H+Tqt0raLuk6Mzt/xm30S/pg+een2zEvAAAWu3Yc6/56SX+h4kh3d0t6lz3/WNYb3f1mSXL3vWb2dhWBf6eZ3aLiELivVfHTu1tVHBYXAADMUzu+dX9KeVqVdMMc1/mepJun/3D328zsMknvVXGI3OmmNn8k6RMzj4sf1dvbr1NOPvPQV5xlyZL0BhODg4PJNZI0OTmZXLN37+7QWHv27A3V7duXPl69MXXoK7VQrabPcXxkLDTW6O5YnZrpzU6ia3Ml0MSlJ9AIR5IiT7lmrDeNvIM9M8NbkmZ64fNbdx2eSHMrSdqyZUtyzdRU7Lk5Mpbe/OX799wTGmv58tgvq2u96U2Wmo3Y82Wgf2lyzaZNB3uju7WR0eB2apZ5B7273yTppkDdPZJ+e77jAwCAudGPHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZa0f3ugVpydCQLrro4uS6iqW/9pmYnEiukaRdu3Ym1/TUYq/NBod6Q3WrV69Irok2H2zW01fH5YFug5JkzUaorhmoawS7+dXr6Z3ymoGua5LUaKTfr4l6evdFSWpE149menu4aLe2ZmB5NKfSHy8pPsctm9O710W3Vaqkb3f2jOwPDTUyHuvY1lNL71432D8cGsuVvj3tHVySXBN8Oj8Pe/QAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxrLtXlepVDQ42BeqSzW0ZCC5RpKOWZ3eOcl1fGgsk4XqQp3ogh2XvJ5eGGyEpkqlGisMLMZGI9jlrZHeDS3ShS5aVw/MT5KajfQudJLknl43ORnrDDc1lf6YTQU7w01MxOoi3Q2nAjWSNKX09SPaeC3a/TKy7R7sTe8oJ0l9/el11d70nPjJI73SSHLZ87BHDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMZdvUxuVqNNIbWtQb6Q0VzGINY3oCbR/MYg1BKlYL1ZnS63qqsdWqWg00s7DYa1ULzrFSTW+GU6nEHrPIahVtXmSV9LpKsGtJbIaxpiWV4PrhgedZoxls8tOMrR+NenqjmUagMZAkNSKLMfhAN4JNjyJNoNQINvtS+nZgPNBvqre3N72oBfboAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIzNu3udma2S9AZJr5b0YknrJE1K+qmkL0j6gvuvWiaZ2cmSNhzkJr/i7tfNd15yyQPNjCrV9G5G1WD3Ogt0ygu1NJPiLcMs0CGrGWjTJCnSs8oCHc0kyYKdxpTeEFGyWJu36EMWGyykZ5b/AAAP8klEQVR9tODdCnPv3ICVQDe/6APWbAbXj8B40U55Hhgs+nh5oKunJDUjXe+a6V3oJCn0kEWWR5tW+Xa0qb1W0qclbZJ0h6SnJK2R9EZJn5X0KjO71p//qP9E0m0tbu+hNswJAACoPUH/qKTXSvrHWXvufyrp3yS9SUXof21W3QPuflMbxgcAAHOY92f07n67u39jZsiX52+W9Jnyz8vnOw4AAEjXjj36g5n+RLPVB6LHmdnvSVolaYek+9z9wSM8HwAAFpUjFvRm1iPpreWf32pxld8s/82suVPS9e7+1GGOcf8cF511mNMEACBrR/LndR+W9CJJ33T3b884f1TSX0o6T9KK8t9lKr7Id7mk75rZ0BGcFwAAi8YR2aM3s3dJeo+kRyS9ZeZl7r5V0p/NKrnLzK6S9H1JF0h6m6SPH2ocdz9vjvHvl3Ru+swBAMhL2/fozeydKkL6YUlXuPvOw6lz97qKn+NJ0qXtnhcAAItRW4PezG6Q9CkVv4W/ovzmfYpt5Slv3QMA0AZtC3oz+2NJH5X0gIqQ3xq4mQvL0yfaNS8AABaztgS9mb1fxZfv7pf0SnfffpDrXmBmvS3Ov1LSu8s/v9yOeQEAsNi141j310v6C0kNSXdLepc9/7jIG9395vL/H5F0dvlTumfK814i6cry/+9393vnOy8AANCeb92fUp5WJd0wx3W+J+nm8v9fUtEE5+WSXiWpJmmLpK9K+pS7392GOUmSKoE3LCJvcUSbj1QsvaFCqNmGpEqw+Uu0rlNjtXhRueCEm3t0sIlLyMJf9OFl2Ah0LYmOFW40ExgvPsfIWJ27X1JsOXojFoFTjfSVf8rTm31F143Z5h305fHqb0q4/uckfW6+4wIAgEOjHz0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMtaO7nULVqTzT6Ua6KCWXDFdl14Z7dYWretkB7VOjtXJrnedfszwK53sHLjguw1qPt3Q0uuazXy3OY1G+vKYatSTa9q1LNijBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDGCHoAADKWbfc6d1c90C1IFnjt0xN7vRTpS9QMNjOqVGJzjNZFRDo1VavVjo0V1cnudUdDx7tOdlLs5FiNRiM0VifXj+h6H9kORMeKLo/Qtiq4fatW0+9bxQPP5+SKOcZu0+0AAIAFiKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMhYtk1tTFIl0KAm0ogh2rwhVBbsxdJsNmOFAUdD05KjoflLxNHQIKWTDYU6qZPrfbTuaFjvO7kcm8EuYc1G+va0Xk9vstauZwp79AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQMYIeAICMEfQAAGSMoAcAIGMEPQAAGSPoAQDIGEEPAEDG2tK9zsw+Iul8SWdIOkbSmKQnJd0m6VPuvqNFzcWS3ifpQkn9kh6X9HlJn3T3RhsmpWq1mlzWU0t/7VOrpY8jSZXAy6xAQ76iroNdzXLukBXRye5k0S6FR8Njhu6JrB+VyAZOUqMx/83/QhTaDrSp02O79ujfLWlI0j9L+rikv5dUl3STpAfN7ISZVzaz10m6S9Klkr4u6a8l9Ur6qKRb2jQnAAAWvXb1ox929/HZZ5rZhyT9qaT/LukPyvOGJf2dpIaky939h+X575d0u6RrzOw6dyfwAQCYp7bs0bcK+dJXy9PTZ5x3jaTVkm6ZDvkZt/G+8s93tGNeAAAsdkf6y3ivKU8fnHHeleXpt1pc/y5Jo5IuNrO+IzkxAAAWg3a9dS9JMrMbJS2RtEzFl/N+XUXIf3jG1c4sTx+dXe/udTPbIOlsSesl/ewQ490/x0Vnpc0cAIA8tTXoJd0oac2Mv78l6T+7+7YZ5y0rT/fMcRvT5y9v89wAAFh02hr07r5WksxsjaSLVezJ/9jM/qO7/+gwb2b6dxyH/F2Bu5/X8gaKPf1zD3M8AACydUQ+o3f3Le7+dUlXSVol6YszLp7eY1/2vMLC8KzrAQCAoCP6ZTx3f1LSw5LONrNjyrN/Xp6eMfv6ZtYj6RQVv8F/4kjODQCAxaATh8A9rjydPtzR7eXp1S2ue6mkQUn3uvvEkZ4YAAC5m3fQm9lZZra2xfmV8oA5x6oI7l3lRbdK2i7pOjM7f8b1+yV9sPzz0/OdFwAAaM+X8a6W9FdmdpekX0jaoeKb95ep+IncZklvn76yu+81s7erCPw7zewWSTslvVbFT+9ulfSVNswLAIBFrx1B//8k/S9Jl0g6R8XP4kZU/E7+S5I+4e47Zxa4+21mdpmk90p6k37V1OaPyuu35Uj+bbqZQwo3jAm8n1KpxMaKNpiI1B0NDVKOhjlG1t9ONtCJ8kP/oKYlUycbEaXXmMWaW/X0xO5XT0/6c7PRrIfGajbS59hsRtfF2BxjTaBiDXQaU+l1zan0+9Wup+W8g97dH5L0zkDdPZJ+e77jAwCAudGPHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgY9bJZhadYmY7enpqK49ZuTpQGxovvSg4VtzRMEcsBtEtTidXxc5uFoNNfkJPzmijmUjNUdBgqRmra4Ya6KTX7NqzQ41Gfae7r0ouniHXoN8gaVjSxhYXn1WePtKxCS1sLI8DsTwOxPI4EMvjQCyPA7V7eZwsaa+7nzKfG8ky6A/GzO6XJHc/r9tzWQhYHgdieRyI5XEglseBWB4HWqjLg8/oAQDIGEEPAEDGCHoAADJG0AMAkDGCHgCAjC26b90DALCYsEcPAEDGCHoAADJG0AMAkDGCHgCAjBH0AABkjKAHACBjBD0AABlbNEFvZseb2efN7DkzmzCzjWb2MTNb0e25dVp5332Of5u7Pb8jwcyuMbNPmtndZra3vK9fPkTNxWb2TTPbaWajZvagmd1gZtVOzftISVkeZnbyQdYXN7NbOj3/djKzVWb2NjP7upk9bmZjZrbHzL5vZr9rZi23k7muH6nLI/f1Q5LM7CNm9l0ze7pcHjvN7Mdm9udm1rJX/EJaP3o6PWA3mNmpku6VdKykf1DRK/gVkv5Q0tVmdom77+jiFLthj6SPtTh/f6cn0iHvk3SOivv3jH7VN7olM3udpK9JGpf0FUk7Jb1G0kclXSLp2iM52Q5IWh6ln0i6rcX5D7VxXt1wraRPS9ok6Q5JT0laI+mNkj4r6VVmdq3POLpY5utH8vIo5bp+SNK7Jf1I0j9L2ippSNKFkm6S9F/M7EJ3f3r6ygtu/XD37P9J+rYkl/RfZ53/P8vzP9PtOXZ4eWyUtLHb8+jwfb5C0umSTNLl5eP+5TmuO6ziyTwh6fwZ5/ereMHokq7r9n3q4PI4ubz85m7P+wgtiytVbIQrs85fqyLkXNKbFsv6EVgeWa8f04/tHOd/qLzvf7OQ14/s37o3s/WSrlIRbn896+I/lzQi6S1mNtThqaGD3P0Od3/My2fcIVwjabWkW9z9hzNuY1zFnrAkveMITLNjEpdH1tz9dnf/hrs3Z52/WdJnyj8vn3FR1utHYHlkr3xsW/lqeXr6jPMW3PqxGN66v7I8/U6LFXefmd2j4oXAhZK+2+nJdVGfmb1Z0okqXuw8KOkud290d1oLwvQ6860Wl90laVTSxWbW5+4TnZtW1x1nZr8naZWkHZLuc/cHuzynI22qPK3POG8xrx+tlse0xbh+vKY8nXk/F9z6sRiC/szy9NE5Ln9MRdCfocUV9GslfWnWeRvM7Hfc/XvdmNACMuc64+51M9sg6WxJ6yX9rJMT67LfLP/9kpndKel6d3+qKzM6gsysR9Jbyz9nbrQX5fpxkOUxLfv1w8xulLRE0jJJ50v6dRUh/+EZV1tw60f2b92reECk4stnrUyfv7wDc1koviDplSrCfkjSiyX9rYrP2v7JzM7p3tQWBNaZA41K+ktJ50laUf67TMUXtS6X9N1MP/r6sKQXSfqmu397xvmLdf2Ya3kspvXjRhUf+d6gIuS/Jekqd9824zoLbv1YDEF/KFaeLprPKt39A+XncFvcfdTdH3L331fx5cQBFd8kxdwW1Trj7lvd/c/c/Ufuvrv8d5eKd8L+VdJpkt7W3Vm2l5m9S9J7VPxC5y2p5eVpNuvHwZbHYlo/3H2tu5uKnaQ3qtgr/7GZnZtwMx1fPxZD0E+/elo2x+XDs663mE1/0ebSrs6i+1hnDoO711X83ErKaJ0xs3dK+rikhyVd4e47Z11lUa0fh7E8Wsp1/ZCkcifp6ypezKyS9MUZFy+49WMxBP3Py9Mz5rh8+tuSc32Gv5hsLU9zeZstas51pvyc8hQVX0Z6opOTWqCm37LMYp0xsxskfUrFb7+vKL9pPtuiWT8Oc3kcTFbrx2zu/qSKF0Bnm9kx5dkLbv1YDEF/R3l6VYsjOi1VcfCCMUn/0umJLUAXladH/QZqnm4vT69ucdmlkgYl3ZvhN6ojLixPj/p1xsz+WMUBTR5QEWpb57jqolg/EpbHwWSzfhzEceXp9C+WFtz6kX3Qu/svJH1HxRfN3jnr4g+oeKX5RXcf6fDUusLMzjazlS3OP0nFK3dJOuihYReBWyVtl3SdmZ0/faaZ9Uv6YPnnp7sxsW4wswvMrLfF+VeqOGKYdJSvM2b2fhVfNrtf0ivdfftBrp79+pGyPHJfP8zsLDNb2+L8ipl9SMURV+91913lRQtu/bDFcLyMFofA/ZmkC1QcHexRSRf7IjkErpndJOlPVLzTsUHSPkmnSnq1iiM3fVPSG9x9sltzPBLM7PWSXl/+uVbSb6nYy7i7PG+7u9846/q3qjiE5S0qDmH5WhU/nblV0n86mg82k7I8yp9InS3pThWHy5Wkl+hXvxd+v7tPb8COOmZ2vaSbVeyRfVKtPzvd6O43z6jJdv1IXR6LYP24QdJfqfgN/C9UHCNgjYpfFqyXtFnFi6GHZ9QsrPWjk4fh6+Y/SSeo+FnZJkmTkp5U8QWTld2eW4eXw2WS/reKb8/uVnEAjG0qjuH8VpUv/nL7p+KXBH6Qfxtb1Fyi4oXPLhUf7/xUxR5Ktdv3p5PLQ9LvSvq/Ko4uuV/FoT2fUnEM79/o9n3pwLJwSXculvUjdXksgvXjRSqOqvqAij31uooXPz8ol1XLDFlI68ei2KMHAGCxyv4zegAAFjOCHgCAjBH0AABkjKAHACBjBD0AABkj6AEAyBhBDwBAxgh6AAAyRtADAJAxgh4AgIwR9AAAZIygBwAgYwQ9AAAZI+gBAMgYQQ8AQMYIegAAMkbQAwCQsf8Pi7zBaIWVzh4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 9999\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "sample_image = features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters =  300\n",
    "batch_size = 64\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.8 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.placeholder(tf.float32, (200,32,32,1))\n",
    "x_image = tf.reshape(a,[-1,32,32,1])\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to restore last checkpoint\n",
      "Failed to restore checkpoint. Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoint'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "try:\n",
    "    print(\"trying to restore last checkpoint\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "\n",
    "    # Try and load the data in the checkpoint.\n",
    "    saver.restore(session, save_path=last_chk_path)\n",
    "\n",
    "    # If we get to this point, the checkpoint was successfully loaded.\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    # If the above failed for some reason, simply\n",
    "    # initialize all the variables for the TensorFlow graph.\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    #layerConvOut = layer\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    #layer += biases\n",
    "\n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_relu(input,              # The previous layer.\n",
    "                  use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=input,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function for a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 1\n",
    "img_size = 32\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_classes = 10\n",
    "\n",
    "# convolution layer1\n",
    "filter_size1 = 3\n",
    "num_filters1 = 64\n",
    "\n",
    "# convolution layer2\n",
    "filter_size2 = 3\n",
    "num_filters2 = 128\n",
    "\n",
    "#convolution layer3\n",
    "filter_size3 = 3\n",
    "num_filters3 = 256\n",
    "\n",
    "\n",
    "fc_size1 = 1024\n",
    "fc_size2 = 1024\n",
    "fc_size3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], \n",
    "                          padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1,biases_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1)\n",
    "\n",
    "layer_conv1_biases = layer_conv1 + biases_conv1\n",
    "pool1 = max_pool('pool1', layer_conv1_biases, k=2)\n",
    "norm1 = norm('norm1', pool1, lsize=4)\n",
    "_dropout = keep_prob\n",
    "norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "layer_conv2, weights_conv2,biases_conv2 = \\\n",
    "    new_conv_layer(input=norm1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2)\n",
    "\n",
    "layer_conv2_biases = layer_conv2 + biases_conv2\n",
    "pool2 = max_pool('pool2', layer_conv2_biases, k=2)\n",
    "    # Apply Normalization\n",
    "norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Apply Dropout\n",
    "norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "layer_conv3, weights_conv3,biases_conv3 = \\\n",
    "    new_conv_layer(input=norm2,\n",
    "                   num_input_channels=num_filters2,\n",
    "                   filter_size=filter_size3,\n",
    "                   num_filters=num_filters3)\n",
    "\n",
    "layer_conv3_biases = layer_conv3 + biases_conv3\n",
    "\n",
    "pool3 = max_pool('pool3', layer_conv3, k=2)\n",
    "    # Apply Normalization\n",
    "norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Apply Dropout\n",
    "norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "layer_flat, num_features = flatten_layer(norm3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1,weights_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size1,\n",
    "                         use_relu=True)\n",
    "layer_flat1, num_features1 = flatten_layer(layer_fc1)\n",
    "layer_fc2,weights_fc2 = new_fc_layer(input=layer_flat1,\n",
    "                         num_inputs=num_features1,\n",
    "                         num_outputs=fc_size2,\n",
    "                         use_relu=True)\n",
    "layer_flat2, num_features2 = flatten_layer(layer_fc2)\n",
    "layer_fc3,weights_fc3 = new_fc_layer(input=layer_flat2,\n",
    "                         num_inputs=num_features2,\n",
    "                         num_outputs=fc_size3,\n",
    "                         use_relu=True)\n",
    "y_pred = tf.nn.softmax(layer_fc3)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_conv1_original = session.run(weights_conv1)\n",
    "w_conv2_original = session.run(weights_conv2)\n",
    "w_conv3_original = session.run(weights_conv3)\n",
    "\n",
    "w_fc1_original = session.run(weights_fc1)\n",
    "w_fc2_original = session.run(weights_fc2)\n",
    "w_fc3_original = session.run(weights_fc3)\n",
    "wOrigConv = [w_conv1_original,w_conv2_original, w_conv3_original]\n",
    "wOrigFc = [w_fc1_original, w_fc2_original, w_fc3_original]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreWeights():\n",
    "    for wIdx in range(0,len(weightsConvMat)):\n",
    "            assign_op = weightsConvMat[wIdx].assign(wOrigConv[wIdx])\n",
    "            session.run(assign_op)\n",
    "\n",
    "    for wIdx in range(0,len(weightsFcMat)):\n",
    "            assign_op = weightsFcMat[wIdx].assign(wOrigFc[wIdx])\n",
    "            session.run(assign_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restoreWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsConvMat = [weights_conv1,weights_conv2, weights_conv3]\n",
    "weightsFcMat = [weights_fc1, weights_fc2, weights_fc3]\n",
    "\n",
    "wConv1 = session.run(weights_conv1)\n",
    "wConv2 = session.run(weights_conv2)\n",
    "wConv3 = session.run(weights_conv3)\n",
    "wFc1 = session.run(weights_fc1)\n",
    "wFc2 = session.run(weights_fc2)\n",
    "wFc3 = session.run(weights_fc3)\n",
    "wConv = [wConv1,wConv2, wConv3]\n",
    "wFc = [wFc1,wFc2, wFc3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## container to hold the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wDiffConv = [] + wOrigConv\n",
    "wDiffFc = [] + wOrigFc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroidFilter(wtListConv,wtListFc):\n",
    "    num = len(wtListConv)\n",
    "    \n",
    "    for wtMat in wtListConv:\n",
    "        shape = wtMat.shape\n",
    "        xlen = shape[0]\n",
    "        ylen = shape[1]\n",
    "        numChnls = shape[2]\n",
    "        numFilters = shape[3]\n",
    "\n",
    "        for c in range(0,numChnls):\n",
    "            for i in range(0,numFilters):\n",
    "                for j in range(0,xlen,2):\n",
    "                    for k in range(0,ylen,2):\n",
    "                        first = wtMat[j,k,c,i]\n",
    "                        if j+1 < xlen:\n",
    "                            second = wtMat[j+1,k,c,i]\n",
    "                        else:\n",
    "                            second = 0\n",
    "                        if k+1 < xlen:\n",
    "                            third = wtMat[j,k+1,c,i]\n",
    "                        else:\n",
    "                            third = 0\n",
    "                        if j+1 < xlen and k+1 < ylen:\n",
    "                            forth = wtMat[j+1,k+1,c,i]\n",
    "                        else:\n",
    "                            forth = 0\n",
    "                        total = 0.0\n",
    "                        total = float(first + second + third + forth)\n",
    "                        total /= 4\n",
    "                        wtMat[j,k,c,i] = total\n",
    "                        if j+1 < xlen:\n",
    "                            wtMat[j+1,k,c,i] = total\n",
    "                        if k+1 < xlen:\n",
    "                            wtMat[j,k+1,c,i] = total\n",
    "                        if j+1 < xlen and k+1 < ylen:\n",
    "                            wtMat[j+1,k+1,c,i] = total\n",
    "                        #if wtMat[j,k,0,i] < 0:\n",
    "                        #    wtMat[j,k,0,i] = 0#float('%.5f'%(w[j,k,0,i]))\n",
    "                        \n",
    "    for wtMat in wtListFc:\n",
    "        shape = wtMat.shape\n",
    "        xlen = shape[0]\n",
    "        ylen = shape[1]\n",
    "        \n",
    "        for i in range(0,xlen):\n",
    "            j=0\n",
    "            while j<ylen:\n",
    "                first = wtMat[i,j] \n",
    "                second = wtMat[i,j+1]\n",
    "                avg = (first+second)/2\n",
    "                wtMat[i,j] = avg\n",
    "                wtMat[i,j+1] = avg\n",
    "                j = j+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroidFilter(wConv, wFc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignFilters(wConv,wFc):\n",
    "    for wIdx in range(0,len(weightsConvMat)):\n",
    "        assign_op = weightsConvMat[wIdx].assign(wConv[wIdx])\n",
    "        session.run(assign_op)\n",
    "        \n",
    "    for wIdx in range(0,len(weightsFcMat)):\n",
    "        assign_op = weightsFcMat[wIdx].assign(wFc[wIdx])\n",
    "        session.run(assign_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignFilters(wConv,wFc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadValues(numImg):\n",
    "    \n",
    "    feed_dict = {a: imageTest, keep_prob:dropout}\n",
    "    values_conv1 = session.run(layer_conv1, feed_dict=feed_dict)\n",
    "    \n",
    "    values_conv2 = session.run(layer_conv2, feed_dict=feed_dict)\n",
    "    values_conv3 = session.run(layer_conv3, feed_dict = feed_dict)\n",
    "    values_fc1 = session.run(layer_fc1, feed_dict = feed_dict)\n",
    "    values_fc2 = session.run(layer_fc2, feed_dict = feed_dict)\n",
    "    values_fc3 = session.run(layer_fc3, feed_dict = feed_dict)\n",
    "    \n",
    "    valuesConvMat = [values_conv1, values_conv2, values_conv3]\n",
    "    valuesFcMat = [values_fc1, values_fc2, values_fc3]\n",
    "    \n",
    "    return valuesConvMat, valuesFcMat, numImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageTest = features[0:200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateConvME(valuesMat):\n",
    "    shape = valuesMat.shape\n",
    "    valMat = valuesMat\n",
    "    numFilters = shape[3]\n",
    "   ## print(\"we are in the calculateConvME function\")\n",
    "   ## print(valMat)\n",
    "    means = np.zeros(numFilters)\n",
    "    meanSum = np.zeros(numFilters)\n",
    "    print(\"the number of filters are\")\n",
    "    print(numFilters)\n",
    "    for nImg in range(0,numImg):\n",
    "        meanSum = meanSum + means\n",
    "        for i in range(0,numFilters):\n",
    "            result = valMat[nImg,:,:,i]\n",
    "            result_sum = np.sum(result)\n",
    "            means[i] = result_sum\n",
    "\n",
    "    average = [x / numImg for x in meanSum]\n",
    "    averageSorted = np.sort(average)\n",
    "    return average, averageSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateFCME(valuesMat):\n",
    "    shape = valuesMat.shape\n",
    "    valMat = valuesMat\n",
    "    numFilters = shape[1]\n",
    "    ##print(\"we are in the fully connected layer function\")\n",
    "    ##print(valMat)\n",
    "    means = np.zeros(numFilters)\n",
    "    meanSum = np.zeros(numFilters)\n",
    "\n",
    "    for nImg in range(0,numImg):\n",
    "        meanSum = meanSum + means\n",
    "        for i in range(0,numFilters):\n",
    "            result = valMat[nImg,i]\n",
    "            result_sum = np.sum(result)\n",
    "            means[i] = result_sum\n",
    "\n",
    "    average = [x / numImg for x in meanSum]\n",
    "    averageSorted = np.sort(average)\n",
    "    return average, averageSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMEConvolution(ME1, ME2, ME3):\n",
    "    x1 = np.arange(len(ME1))\n",
    "    x2 = np.arange(len(ME2))\n",
    "    x3 = np.arange(len(ME3))\n",
    "    \n",
    "    plt.bar(x1, ME1, color = 'b', align = 'center')\n",
    "    d1=  len(ME1) + 1\n",
    "    plt.bar(x2+d1, ME2, color = 'g', align = 'center')\n",
    "    d2 = d1 + len(ME2) + 1\n",
    "    #location = x2 + len(ME1) \n",
    "    plt.bar(x3+d2, ME3, color = 'r', align = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMEFullyConnected(ME1, ME2, ME3):\n",
    "    x1 = np.arange(len(ME1))\n",
    "    x2 = np.arange(len(ME2))\n",
    "    x3 = np.arange(len(ME3))\n",
    "    \n",
    "    plt.bar(x1, ME1, color = 'b', align = 'center')\n",
    "    d1=  len(ME1) + 1\n",
    "    plt.bar(x2+d1, ME2, color = 'g', align = 'center')\n",
    "    d2 = d1 + len(ME2) + 1\n",
    "    #location = x2 + len(ME1) \n",
    "    plt.bar(x3+d2, ME3, color = 'r', align = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (200, 32, 32, 3) for Tensor 'Placeholder_3:0', which has shape '(100, 28, 28, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-9198bfa6ce91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloadValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-4dcfaafb7d8f>\u001b[0m in \u001b[0;36mloadValues\u001b[0;34m(numImg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimageTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvalues_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues_conv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (200, 32, 32, 3) for Tensor 'Placeholder_3:0', which has shape '(100, 28, 28, 1)'"
     ]
    }
   ],
   "source": [
    "loadValues(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
